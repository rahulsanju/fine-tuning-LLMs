{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3343571,"sourceType":"datasetVersion","datasetId":1987515},{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":30498,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets evaluate rouge_score\n!pip install --upgrade transformers accelerate","metadata":{"id":"ru6PzrU4IW5s","outputId":"53815fd6-ca05-4704-c925-41d1c5090499","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-30T02:03:50.044092Z","iopub.execute_input":"2024-03-30T02:03:50.044356Z","iopub.status.idle":"2024-03-30T02:04:30.570974Z","shell.execute_reply.started":"2024-03-30T02:03:50.044331Z","shell.execute_reply":"2024-03-30T02:04:30.569714Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m983.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (10.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.14.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=479cbdd63d32d846f8f031cf96ba4a17bb2f5beccf97085e5b28cb4ec904dbbc\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score, evaluate\nSuccessfully installed evaluate-0.4.1 rouge_score-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.29.2)\nCollecting transformers\n  Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\nCollecting accelerate\n  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nCollecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nCollecting tokenizers<0.19,>=0.14 (from transformers)\n  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting safetensors>=0.4.1 (from transformers)\n  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nInstalling collected packages: safetensors, huggingface-hub, tokenizers, accelerate, transformers\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.3.1\n    Uninstalling safetensors-0.3.1:\n      Successfully uninstalled safetensors-0.3.1\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.14.1\n    Uninstalling huggingface-hub-0.14.1:\n      Successfully uninstalled huggingface-hub-0.14.1\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.29.2\n    Uninstalling transformers-4.29.2:\n      Successfully uninstalled transformers-4.29.2\nSuccessfully installed accelerate-0.28.0 huggingface-hub-0.22.2 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{"id":"hP74KXAzIW5v"}},{"cell_type":"code","source":"\n# Set hyperparameters\nBATCH_SIZE = 4\nNUM_TRAIN_EPOCHS = 3\nLEARNING_RATE = 3e-5\nWEIGHT_DECAY = 0.01\nMAX_SOURCE_LENGTH = 512\nMAX_TARGET_LENGTH = 64\nTRAINING_DATASET_SIZE = 100\nTESTING_DATASET_SIZE = 10\n\nDATASET_PATH = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\"\nTEST_DATASET_PATH = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv\"\n\n#CHANGE THIS BEFORE ANY HYPERPARAMETER CHANGE !!!!!!!!!!!!\nOUTPUT_DIR_CHECKPOINT = \"/kaggle/working/model_testing_10k_6_14_2023_1\"","metadata":{"id":"yrmfkVTwLTuD","execution":{"iopub.status.busy":"2024-03-30T02:04:30.573166Z","iopub.execute_input":"2024-03-30T02:04:30.573471Z","iopub.status.idle":"2024-03-30T02:04:30.579432Z","shell.execute_reply.started":"2024-03-30T02:04:30.573439Z","shell.execute_reply":"2024-03-30T02:04:30.578606Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load BillSum dataset\nLoading the BillSum dataset with TRAINING_DATASET_SIZE and TESTING_DATASET_SIZE","metadata":{"id":"EiFD_X2gIW5y"}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the dataset\nbillsum = load_dataset(\"csv\", data_files={\"train\": DATASET_PATH, \"test\": TEST_DATASET_PATH})\n\nbillsum[\"train\"] = billsum[\"train\"].select(range(TRAINING_DATASET_SIZE))\nbillsum[\"test\"] = billsum[\"test\"].select(range(TESTING_DATASET_SIZE))","metadata":{"id":"VqyEiePtIW5z","outputId":"e684e917-7dfd-45a0-c3e2-69823720de30","execution":{"iopub.status.busy":"2024-03-30T02:04:35.523060Z","iopub.execute_input":"2024-03-30T02:04:35.523693Z","iopub.status.idle":"2024-03-30T02:05:08.464388Z","shell.execute_reply.started":"2024-03-30T02:04:35.523659Z","shell.execute_reply":"2024-03-30T02:05:08.463518Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-d31c9e8e00e39157/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"895eb904b0d84786bbabf39f252a2046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f55c39211c5400c937b5c6cd5babfbc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-d31c9e8e00e39157/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4062eb5931264ce6bca81248d3c09a01"}},"metadata":{}}]},{"cell_type":"markdown","source":"Split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:","metadata":{"id":"AffOG5pqIW50"}},{"cell_type":"code","source":"#billsum = billsum.train_test_split(test_size=0.2)","metadata":{"id":"9lfNCz9LIW50","execution":{"iopub.status.busy":"2024-03-30T02:05:22.943337Z","iopub.execute_input":"2024-03-30T02:05:22.943973Z","iopub.status.idle":"2024-03-30T02:05:22.948001Z","shell.execute_reply.started":"2024-03-30T02:05:22.943942Z","shell.execute_reply":"2024-03-30T02:05:22.947025Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"In billsum dataset, There are two fields that you'll want to use:\n\n- `clean_text`: the preprocessed text of the bill which'll be the input to the model.\n- `summary`: a condensed version of `text` which'll be the model target.","metadata":{"id":"0bXa-BW6IW50"}},{"cell_type":"code","source":"billsum[\"train\"][0]","metadata":{"id":"BKUJBHG1IW50","outputId":"edeef325-8cac-4510-84d9-d6a48ab7c83c","execution":{"iopub.status.busy":"2024-03-30T02:05:23.689116Z","iopub.execute_input":"2024-03-30T02:05:23.689851Z","iopub.status.idle":"2024-03-30T02:05:23.697229Z","shell.execute_reply.started":"2024-03-30T02:05:23.689815Z","shell.execute_reply":"2024-03-30T02:05:23.696247Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'id': '0001d1afc246a7964130f43ae940af6bc6c57f01',\n 'article': \"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\",\n 'highlights': 'Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\\nHe contracted the infection through contaminated food in Italy .\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess\nThe next step is to load a `bart` tokenizer to process `text` and `summary`:","metadata":{"id":"oJr-08yLIW51"}},{"cell_type":"code","source":"import os\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ncheckpoint_dir = OUTPUT_DIR_CHECKPOINT\n\ncheckpoint = \"facebook/bart-large-cnn\"\n\nif os.path.exists(checkpoint_dir):\n    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_dir)\n    print(\"Using checkpoint model: \", checkpoint_dir)\nelse:\n    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"id":"UplZSWRCIW51","outputId":"58e33ce8-e053-466a-d88e-77e35e93c492","execution":{"iopub.status.busy":"2024-03-30T02:05:24.695580Z","iopub.execute_input":"2024-03-30T02:05:24.696193Z","iopub.status.idle":"2024-03-30T02:05:41.833091Z","shell.execute_reply.started":"2024-03-30T02:05:24.696156Z","shell.execute_reply":"2024-03-30T02:05:41.832109Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3110acda574c4bad322ebfa9d0aca9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73992965dfd94747a714cfe2662ff7b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31417664d46d4f28be66c2c4a75658ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c03e0933f8134eee90f71593a00bdf07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b5a6fb5b8a4086b1eac3d9f2107e59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b10c9e0201429aac54010a681bf662"}},"metadata":{}}]},{"cell_type":"markdown","source":"The preprocessing function you want to create needs to:\n\n1. Prefix the input with a prompt so `facebook/bart-large-cnn` knows this is a summarization task.\n2. Use the keyword `text_target` argument when tokenizing labels.\n3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter.","metadata":{"id":"OhHJR2nyIW51"}},{"cell_type":"code","source":"prefix = \"summarize: \"\n\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=MAX_SOURCE_LENGTH, truncation=True)\n\n    labels = tokenizer(text_target=examples[\"highlights\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"id":"IZ-9HKl7IW51","execution":{"iopub.status.busy":"2024-03-30T02:07:25.446313Z","iopub.execute_input":"2024-03-30T02:07:25.446659Z","iopub.status.idle":"2024-03-30T02:07:25.452525Z","shell.execute_reply.started":"2024-03-30T02:07:25.446634Z","shell.execute_reply":"2024-03-30T02:07:25.451515Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Apply the preprocessing function over the entire dataset method and speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:","metadata":{"id":"Yn2q8Xw9IW51"}},{"cell_type":"code","source":"tokenized_billsum = billsum.map(preprocess_function, batched=True)","metadata":{"id":"SHt0AoplIW51","outputId":"442f3f7d-3683-4f79-b655-4a9ae9256a48","execution":{"iopub.status.busy":"2024-03-30T02:07:28.321453Z","iopub.execute_input":"2024-03-30T02:07:28.321821Z","iopub.status.idle":"2024-03-30T02:07:28.572207Z","shell.execute_reply.started":"2024-03-30T02:07:28.321792Z","shell.execute_reply":"2024-03-30T02:07:28.571248Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4312fd239ea45ea8f687eef3b7a9074"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a885d8e7e95e40408c03f6e2a8780f77"}},"metadata":{}}]},{"cell_type":"markdown","source":"Creating a batch of examples using `DataCollatorForSeq2Seq` which dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.","metadata":{"id":"FoRbnbJDIW51"}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)","metadata":{"id":"78kpGOnGIW51","execution":{"iopub.status.busy":"2024-03-30T02:07:31.107933Z","iopub.execute_input":"2024-03-30T02:07:31.108753Z","iopub.status.idle":"2024-03-30T02:07:39.212901Z","shell.execute_reply.started":"2024-03-30T02:07:31.108722Z","shell.execute_reply":"2024-03-30T02:07:39.212085Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Writing The Evaluate function\nLoaded the `ROUGE` metric:","metadata":{"id":"yDEaCwniIW51"}},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load(\"rouge\")","metadata":{"id":"5wsBLiVvIW52","execution":{"iopub.status.busy":"2024-03-30T02:07:39.214749Z","iopub.execute_input":"2024-03-30T02:07:39.215064Z","iopub.status.idle":"2024-03-30T02:07:40.401480Z","shell.execute_reply.started":"2024-03-30T02:07:39.215011Z","shell.execute_reply":"2024-03-30T02:07:40.400424Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a2580620a84cdb80726e18df9a76a5"}},"metadata":{}}]},{"cell_type":"markdown","source":"Then use `compute_metrics` for the bill sum predictions and labels to `compute` to calculate the ROUGE metric:","metadata":{"id":"4Q-MLlhrIW52"}},{"cell_type":"code","source":"import numpy as np\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"id":"SNp7z7GTIW52","execution":{"iopub.status.busy":"2024-03-30T02:07:40.402739Z","iopub.execute_input":"2024-03-30T02:07:40.403053Z","iopub.status.idle":"2024-03-30T02:07:40.410480Z","shell.execute_reply.started":"2024-03-30T02:07:40.403009Z","shell.execute_reply":"2024-03-30T02:07:40.409426Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Train\nTraining the `facebook\\bart-large-cnn` model using AutoModelForSeq2SeqLM which loads the pretrained model","metadata":{"id":"PMnVoxKEIW52"}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)","metadata":{"id":"VXndezREIW52","execution":{"iopub.status.busy":"2024-03-30T02:07:40.412552Z","iopub.execute_input":"2024-03-30T02:07:40.412849Z","iopub.status.idle":"2024-03-30T02:07:42.427364Z","shell.execute_reply.started":"2024-03-30T02:07:40.412823Z","shell.execute_reply":"2024-03-30T02:07:42.426204Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"At this point, only three steps remain:\n\n1. Using `Seq2SeqTrainingArguments`, we can configure the hyperparameter for the model, At the end of each step, the `trainer` will evaluate the ROUGE metric and save the training checkpoint.\n2. Pass the training arguments to `Seq2SeqTrainer` along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call `train()` and `save_model()` to train and save the model.","metadata":{"id":"CQZ_kovRIW52"}},{"cell_type":"code","source":"from transformers.trainer_callback import EarlyStoppingCallback\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nif not os.path.exists(checkpoint_dir):\n    # Set up the training arguments\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=checkpoint_dir,\n        evaluation_strategy=\"steps\",  # Change evaluation strategy to \"steps\"\n        learning_rate=LEARNING_RATE,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        weight_decay=WEIGHT_DECAY,\n        save_total_limit=1,\n        num_train_epochs=NUM_TRAIN_EPOCHS,\n        predict_with_generate=True,\n        fp16=True,\n        logging_steps=1,\n        load_best_model_at_end=True\n    )\n\n    # Set up the trainer with early stopping\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_billsum[\"train\"],\n        eval_dataset=tokenized_billsum[\"test\"],\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n    )\n\n    # Train the model\n    trainer.train()\n    trainer.save_model(checkpoint_dir)\nelse:\n    print(\"Checkpoint already exists. Skipping training.\")","metadata":{"id":"1QDR2zkOIW52","outputId":"dc7b7ccb-a1e2-42ae-fd7d-7eabe862c6fd","execution":{"iopub.status.busy":"2024-03-30T02:07:42.428716Z","iopub.execute_input":"2024-03-30T02:07:42.429028Z","iopub.status.idle":"2024-03-30T02:12:53.780155Z","shell.execute_reply.started":"2024-03-30T02:07:42.429000Z","shell.execute_reply":"2024-03-30T02:12:53.779305Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [39/39 05:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.884300</td>\n      <td>2.424106</td>\n      <td>0.492500</td>\n      <td>0.262000</td>\n      <td>0.354200</td>\n      <td>0.418000</td>\n      <td>78.600000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.328900</td>\n      <td>2.179036</td>\n      <td>0.486700</td>\n      <td>0.272400</td>\n      <td>0.348400</td>\n      <td>0.433400</td>\n      <td>78.100000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.102600</td>\n      <td>2.092962</td>\n      <td>0.480000</td>\n      <td>0.265100</td>\n      <td>0.358300</td>\n      <td>0.415700</td>\n      <td>71.600000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.688800</td>\n      <td>2.021408</td>\n      <td>0.487700</td>\n      <td>0.252500</td>\n      <td>0.359700</td>\n      <td>0.413200</td>\n      <td>72.100000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.098000</td>\n      <td>1.967266</td>\n      <td>0.494500</td>\n      <td>0.249900</td>\n      <td>0.346900</td>\n      <td>0.409700</td>\n      <td>69.700000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.765800</td>\n      <td>1.934508</td>\n      <td>0.477800</td>\n      <td>0.249700</td>\n      <td>0.351800</td>\n      <td>0.416200</td>\n      <td>69.900000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.713000</td>\n      <td>1.895838</td>\n      <td>0.459900</td>\n      <td>0.244500</td>\n      <td>0.346600</td>\n      <td>0.402700</td>\n      <td>72.200000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.580300</td>\n      <td>1.859030</td>\n      <td>0.468500</td>\n      <td>0.250800</td>\n      <td>0.358000</td>\n      <td>0.410700</td>\n      <td>72.600000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.311800</td>\n      <td>1.825379</td>\n      <td>0.467800</td>\n      <td>0.241200</td>\n      <td>0.338000</td>\n      <td>0.408200</td>\n      <td>71.700000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.691300</td>\n      <td>1.787329</td>\n      <td>0.444300</td>\n      <td>0.216100</td>\n      <td>0.331500</td>\n      <td>0.390600</td>\n      <td>68.400000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.830400</td>\n      <td>1.754415</td>\n      <td>0.462400</td>\n      <td>0.220100</td>\n      <td>0.335400</td>\n      <td>0.393500</td>\n      <td>68.400000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.839700</td>\n      <td>1.719270</td>\n      <td>0.468600</td>\n      <td>0.219700</td>\n      <td>0.328700</td>\n      <td>0.392600</td>\n      <td>71.000000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.290400</td>\n      <td>1.692935</td>\n      <td>0.447600</td>\n      <td>0.214200</td>\n      <td>0.327500</td>\n      <td>0.389400</td>\n      <td>69.500000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.183300</td>\n      <td>1.671992</td>\n      <td>0.458300</td>\n      <td>0.230000</td>\n      <td>0.344200</td>\n      <td>0.398200</td>\n      <td>68.400000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.023800</td>\n      <td>1.652223</td>\n      <td>0.461400</td>\n      <td>0.229200</td>\n      <td>0.344900</td>\n      <td>0.397900</td>\n      <td>67.700000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.406100</td>\n      <td>1.637087</td>\n      <td>0.456200</td>\n      <td>0.225000</td>\n      <td>0.334200</td>\n      <td>0.393300</td>\n      <td>73.200000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.815800</td>\n      <td>1.623798</td>\n      <td>0.454600</td>\n      <td>0.236700</td>\n      <td>0.329900</td>\n      <td>0.389000</td>\n      <td>70.600000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.985300</td>\n      <td>1.612977</td>\n      <td>0.444200</td>\n      <td>0.229200</td>\n      <td>0.322100</td>\n      <td>0.384500</td>\n      <td>87.400000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.083600</td>\n      <td>1.610624</td>\n      <td>0.477800</td>\n      <td>0.247800</td>\n      <td>0.332000</td>\n      <td>0.433700</td>\n      <td>94.200000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.142400</td>\n      <td>1.615999</td>\n      <td>0.451900</td>\n      <td>0.213900</td>\n      <td>0.332700</td>\n      <td>0.416000</td>\n      <td>90.200000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.405000</td>\n      <td>1.619538</td>\n      <td>0.449300</td>\n      <td>0.206800</td>\n      <td>0.312900</td>\n      <td>0.421200</td>\n      <td>101.500000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.125800</td>\n      <td>1.619467</td>\n      <td>0.448100</td>\n      <td>0.206700</td>\n      <td>0.312400</td>\n      <td>0.418400</td>\n      <td>96.600000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.078400</td>\n      <td>1.618332</td>\n      <td>0.457700</td>\n      <td>0.219600</td>\n      <td>0.318700</td>\n      <td>0.423200</td>\n      <td>99.200000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.905600</td>\n      <td>1.615877</td>\n      <td>0.469400</td>\n      <td>0.218800</td>\n      <td>0.320500</td>\n      <td>0.428000</td>\n      <td>97.300000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.942400</td>\n      <td>1.614557</td>\n      <td>0.477100</td>\n      <td>0.222700</td>\n      <td>0.333500</td>\n      <td>0.439400</td>\n      <td>80.500000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.828200</td>\n      <td>1.615322</td>\n      <td>0.498800</td>\n      <td>0.249300</td>\n      <td>0.343600</td>\n      <td>0.464000</td>\n      <td>83.000000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.745400</td>\n      <td>1.619973</td>\n      <td>0.465200</td>\n      <td>0.228500</td>\n      <td>0.334400</td>\n      <td>0.429400</td>\n      <td>75.600000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.055000</td>\n      <td>1.622993</td>\n      <td>0.470300</td>\n      <td>0.217600</td>\n      <td>0.327800</td>\n      <td>0.430400</td>\n      <td>74.800000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.955300</td>\n      <td>1.624992</td>\n      <td>0.458400</td>\n      <td>0.213200</td>\n      <td>0.326100</td>\n      <td>0.418700</td>\n      <td>75.000000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.669400</td>\n      <td>1.627142</td>\n      <td>0.463400</td>\n      <td>0.213100</td>\n      <td>0.323500</td>\n      <td>0.427900</td>\n      <td>78.600000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.602100</td>\n      <td>1.629622</td>\n      <td>0.462700</td>\n      <td>0.216200</td>\n      <td>0.323200</td>\n      <td>0.426800</td>\n      <td>86.700000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.824200</td>\n      <td>1.632529</td>\n      <td>0.456800</td>\n      <td>0.212600</td>\n      <td>0.322700</td>\n      <td>0.422700</td>\n      <td>88.300000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.583600</td>\n      <td>1.635508</td>\n      <td>0.459200</td>\n      <td>0.219600</td>\n      <td>0.305400</td>\n      <td>0.424800</td>\n      <td>92.600000</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.761100</td>\n      <td>1.638161</td>\n      <td>0.464400</td>\n      <td>0.221000</td>\n      <td>0.312800</td>\n      <td>0.434200</td>\n      <td>95.200000</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.890700</td>\n      <td>1.640509</td>\n      <td>0.464600</td>\n      <td>0.219800</td>\n      <td>0.311600</td>\n      <td>0.431600</td>\n      <td>95.800000</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.622700</td>\n      <td>1.642238</td>\n      <td>0.460500</td>\n      <td>0.217200</td>\n      <td>0.311900</td>\n      <td>0.428900</td>\n      <td>99.200000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.765100</td>\n      <td>1.643307</td>\n      <td>0.467800</td>\n      <td>0.219400</td>\n      <td>0.311200</td>\n      <td>0.432600</td>\n      <td>97.200000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.753900</td>\n      <td>1.644136</td>\n      <td>0.470700</td>\n      <td>0.221300</td>\n      <td>0.307600</td>\n      <td>0.434500</td>\n      <td>93.600000</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.979600</td>\n      <td>1.644369</td>\n      <td>0.458800</td>\n      <td>0.213800</td>\n      <td>0.297100</td>\n      <td>0.418200</td>\n      <td>94.800000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluation on training dataset\ntrain_metrics = trainer.evaluate(eval_dataset=tokenized_billsum[\"train\"])\nprint(\"Training ROUGE Scores:\")\nprint(\"ROUGE-1:\", train_metrics[\"eval_rouge1\"])\nprint(\"ROUGE-2:\", train_metrics[\"eval_rouge2\"])\nprint(\"ROUGE-L:\", train_metrics[\"eval_rougeL\"])\n\n# Evaluation on testing dataset\ntest_metrics = trainer.evaluate(eval_dataset=tokenized_billsum[\"test\"])\nprint(\"Testing ROUGE Scores:\")\nprint(\"ROUGE-1:\", test_metrics[\"eval_rouge1\"])\nprint(\"ROUGE-2:\", test_metrics[\"eval_rouge2\"])\nprint(\"ROUGE-L:\", test_metrics[\"eval_rougeL\"])","metadata":{"id":"NkiRD-TwTD3J","outputId":"d6fd9fef-c4ca-44f3-c0e3-806f23a5a742","execution":{"iopub.status.busy":"2024-03-30T02:12:53.781276Z","iopub.execute_input":"2024-03-30T02:12:53.781599Z","iopub.status.idle":"2024-03-30T02:13:58.914163Z","shell.execute_reply.started":"2024-03-30T02:12:53.781572Z","shell.execute_reply":"2024-03-30T02:13:58.912869Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 01:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training ROUGE Scores:\nROUGE-1: 0.5352\nROUGE-2: 0.3666\nROUGE-L: 0.4415\nTesting ROUGE Scores:\nROUGE-1: 0.4588\nROUGE-2: 0.2138\nROUGE-L: 0.2971\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference\nFor `facebook/bart-large-cnn`, you need to prefix your input depending on the task you're working on. For summarization you should prefix your input as shown below:","metadata":{"id":"xF5UKa1kIW53"}},{"cell_type":"code","source":"text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"","metadata":{"id":"TSzYfMq7IW53","execution":{"iopub.status.busy":"2024-03-30T02:13:58.915385Z","iopub.execute_input":"2024-03-30T02:13:58.915712Z","iopub.status.idle":"2024-03-30T02:13:58.920345Z","shell.execute_reply.started":"2024-03-30T02:13:58.915681Z","shell.execute_reply":"2024-03-30T02:13:58.919362Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Using `summarize_text`, we can tokenize the text and use the `input_ids` for generating summary from the model with `model.generate()`:","metadata":{"id":"k7P8k1OOIW57"}},{"cell_type":"code","source":"import torch\n\ndef summarize_text(input_text):\n\n    text = \"summarize: \" + input_text\n\n    inputs = tokenizer(text, max_length=MAX_SOURCE_LENGTH, truncation=True, return_tensors=\"pt\").input_ids\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n    # Move the model to the GPU\n    model.to(device)\n\n    # Move the inputs tensor to the GPU\n    inputs = inputs.to(device)\n\n    # Generate outputs\n    outputs = model.generate(inputs, max_new_tokens=MAX_TARGET_LENGTH, do_sample=False)\n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return summary\n","metadata":{"id":"0yIedQZqLTuJ","execution":{"iopub.status.busy":"2024-03-30T02:13:58.923729Z","iopub.execute_input":"2024-03-30T02:13:58.924387Z","iopub.status.idle":"2024-03-30T02:13:58.940360Z","shell.execute_reply.started":"2024-03-30T02:13:58.924357Z","shell.execute_reply":"2024-03-30T02:13:58.939428Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Using the first sample of `train` dataset","metadata":{"id":"EPmXcc4-LTuJ"}},{"cell_type":"code","source":"print(billsum[\"train\"][0][\"highlights\"])","metadata":{"id":"vaRdKgpVWuk-","outputId":"eadc56d9-354a-4c1c-d0a3-61130e85a03f","execution":{"iopub.status.busy":"2024-03-30T02:14:41.979484Z","iopub.execute_input":"2024-03-30T02:14:41.980272Z","iopub.status.idle":"2024-03-30T02:14:41.985612Z","shell.execute_reply.started":"2024-03-30T02:14:41.980238Z","shell.execute_reply":"2024-03-30T02:14:41.984540Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\nHe contracted the infection through contaminated food in Italy .\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .\n","output_type":"stream"}]},{"cell_type":"code","source":"summarize_text(billsum[\"train\"][0][\"article\"])","metadata":{"id":"dMtL00ihLTuJ","outputId":"eab14d9d-6bc0-4e3f-98b4-3aa8103623cf","execution":{"iopub.status.busy":"2024-03-30T02:14:46.508768Z","iopub.execute_input":"2024-03-30T02:14:46.509377Z","iopub.status.idle":"2024-03-30T02:14:47.848256Z","shell.execute_reply.started":"2024-03-30T02:14:46.509342Z","shell.execute_reply":"2024-03-30T02:14:47.847282Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Bishop John Folda of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus.\\nThe state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion.\\nB'"},"metadata":{}}]},{"cell_type":"markdown","source":"Using the first sample of `test` dataset","metadata":{"id":"_klaimr7LTuK"}},{"cell_type":"code","source":"print(billsum[\"test\"][0][\"highlights\"])","metadata":{"id":"GGlW5ZkzXzVL","outputId":"e358787f-5017-4f30-bdb5-db3de4984bf6","execution":{"iopub.status.busy":"2024-03-30T02:15:02.933124Z","iopub.execute_input":"2024-03-30T02:15:02.933504Z","iopub.status.idle":"2024-03-30T02:15:02.939175Z","shell.execute_reply.started":"2024-03-30T02:15:02.933475Z","shell.execute_reply":"2024-03-30T02:15:02.938179Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Experts question if  packed out planes are putting passengers at risk .\nU.S consumer advisory group says minimum space must be stipulated .\nSafety tests conducted on planes with more leg room than airlines offer .\n","output_type":"stream"}]},{"cell_type":"code","source":"summarize_text(billsum[\"train\"][0][\"article\"])","metadata":{"id":"6-sAtvroIW57","outputId":"a1ae9bb9-7389-44f5-9aee-d172f56d4154","execution":{"iopub.status.busy":"2024-03-30T02:27:48.426786Z","iopub.execute_input":"2024-03-30T02:27:48.427198Z","iopub.status.idle":"2024-03-30T02:27:49.810868Z","shell.execute_reply.started":"2024-03-30T02:27:48.427166Z","shell.execute_reply":"2024-03-30T02:27:49.809881Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"'Bishop John Folda of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus.\\nThe state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion.\\nB'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install rogue","metadata":{"execution":{"iopub.status.busy":"2024-03-30T02:16:59.585936Z","iopub.execute_input":"2024-03-30T02:16:59.586672Z","iopub.status.idle":"2024-03-30T02:17:13.039029Z","shell.execute_reply.started":"2024-03-30T02:16:59.586630Z","shell.execute_reply":"2024-03-30T02:17:13.037806Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rogue\n  Downloading rogue-0.0.2.tar.gz (5.4 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: rogue\n  Building wheel for rogue (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rogue: filename=rogue-0.0.2-py3-none-any.whl size=7231 sha256=218ca957bc20199e3751199748a9e0d3ce4c3c554f28257b87adfe390b8e617e\n  Stored in directory: /root/.cache/pip/wheels/11/db/ee/a44bd5d88bd4903f31c1f677a28ac79e354555bca48332f232\nSuccessfully built rogue\nInstalling collected packages: rogue\nSuccessfully installed rogue-0.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from rouge import Rouge\ndef calculate_rouge_score(ref_summ, gen_summ):\n    rouge = Rouge()\n\n    # Calculate ROUGE scores\n    scores = rouge.get_scores(gen_summ, ref_summ)\n\n    # Extract ROUGE-1, ROUGE-2, and ROUGE-L scores\n    rouge_1_score = scores[0][\"rouge-1\"][\"f\"]\n    rouge_2_score = scores[0][\"rouge-2\"][\"f\"]\n    rouge_l_score = scores[0][\"rouge-l\"][\"f\"]\n    return rouge_1_score, rouge_2_score, rouge_l_score","metadata":{"execution":{"iopub.status.busy":"2024-03-30T02:27:13.386932Z","iopub.execute_input":"2024-03-30T02:27:13.387345Z","iopub.status.idle":"2024-03-30T02:27:13.440520Z","shell.execute_reply.started":"2024-03-30T02:27:13.387312Z","shell.execute_reply":"2024-03-30T02:27:13.439317Z"},"trusted":true},"execution_count":38,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rouge\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_rouge_score\u001b[39m(ref_summ, gen_summ):\n\u001b[1;32m      3\u001b[0m     rouge \u001b[38;5;241m=\u001b[39m Rouge()\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge'"],"ename":"ModuleNotFoundError","evalue":"No module named 'rouge'","output_type":"error"}]},{"cell_type":"code","source":"text=\"Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDBÂ page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .\"","metadata":{"execution":{"iopub.status.busy":"2024-03-30T02:20:54.018777Z","iopub.execute_input":"2024-03-30T02:20:54.019468Z","iopub.status.idle":"2024-03-30T02:20:54.024879Z","shell.execute_reply.started":"2024-03-30T02:20:54.019434Z","shell.execute_reply":"2024-03-30T02:20:54.023798Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"summarize_text(text)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T02:21:13.496378Z","iopub.execute_input":"2024-03-30T02:21:13.497114Z","iopub.status.idle":"2024-03-30T02:21:14.888157Z","shell.execute_reply.started":"2024-03-30T02:21:13.497069Z","shell.execute_reply":"2024-03-30T02:21:14.887185Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer.\\nForrest was a protege of Hollywood trailblazer Ida Lupino, who cast her in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful.'"},"metadata":{}}]},{"cell_type":"code","source":"text2=\"A middle-school teacher in China has inked hundreds of sketches that are beyond be-leaf. Politics teacher Wang Lian, 35,  has created 1000 stunning ink drawings covering subjects as varied as cartoon characters and landscapes to animals, birds according toÂ theÂ People's Daily Online. The intricate scribbles on leaves feature Wang's favourite sites across the city of Nanjing, which include the Presidential Palace, Yangtze River Bridge, the ancient Jiming Temple and the Qinhuai River. Natural canvas: Artist and teacher Wang Lian has done hundreds of drawings, like this temple, on leaves she collects in the park and on the streets . Delicate: She uses an ink pen to gently draw the local scenes and buildings on the dried out leaves . 'Although teaching politics is my job, drawing is my passion and hobby,' said Wang. 'I first tried drawing on leaves about 10 years ago and fell in love with it as an art form immediately. 'It's like drawing on very old parchment paper, you have to be really careful that you don't damage the leaf because it is very fragile and this helps focus your attention and abilities.' Wang started giving the drawings away onÂ Christmas Eve in 2012 when her junior high school son came home saying he wanted to prepare some gifts for his classmates. Being an avid painter, Wang decided to give her son's friends unique presents of gingko leaf paintings. Wang loves gingko leaves and will often pick them up along Gingko Avenue, near to her school, in Nanjing in east China's Jiangsu province. Every autumn she collects about 2,000 leaves from the ground to ensure she has enough to cover spoils too. Intricate: Teacher Wang has drawn hundreds of local scenes on leaves she has collected from the park . Hobby: The artist collects leaves every autumn and dries them out so she can sketch these impressive building scenes . 'The colour and shape of gingko leaves are particularly beautiful,' she said. 'I need to collect around 2000 leaves because this will include losses'. She takes them home where she then presses them between the pages of books. 'Luckily, I have quite a lot of books and I try to use old ones or ones that I've already read so I don't end up with nothing to read.' Once they are dried, she carefully takes each one and using an ink fountain pen creates her masterpieces. She said: 'Some people are into capturing beauty through photography, but for me, a digitalised image just isn't the same. New leaf:Â Politics teacher Wang Lian has drawn hundreds of doodles on leaves for the last 10 years . 'By drawing what I see I become far more a part of the process and part of the final piece. 'One day I hope to be able to put my collection on display, but for now it's really just for my own pleasure.' Wang's leaf paintings are turned into bookmarks, postcards and sometimes even given as gifts to her her students so she can share the beauty of leaf paintings. But locals who have had the luck of being able to see Wang's art have been gobsmacked. Local art collector On Hao, 58, said: 'These are truly remarkable and beautiful creations. 'She has so much talent she is wasted in teaching.'\"","metadata":{"execution":{"iopub.status.busy":"2024-03-30T02:21:29.973308Z","iopub.execute_input":"2024-03-30T02:21:29.973659Z","iopub.status.idle":"2024-03-30T02:21:29.980267Z","shell.execute_reply.started":"2024-03-30T02:21:29.973632Z","shell.execute_reply":"2024-03-30T02:21:29.979351Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"summarize_text(text2)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T02:21:35.771659Z","iopub.execute_input":"2024-03-30T02:21:35.772023Z","iopub.status.idle":"2024-03-30T02:21:37.172497Z","shell.execute_reply.started":"2024-03-30T02:21:35.771992Z","shell.execute_reply":"2024-03-30T02:21:37.171608Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'Wang Lian, 35, has created 1000 stunning ink drawings on leaves in Nanjing, China.\\nThe political teacher collects leaves every autumn and dries them out so she can draw them.\\nShe uses an ink fountain pen to create intricate designs on the delicate leaves.\\nHer favourite sites'"},"metadata":{}}]},{"cell_type":"code","source":"text3=\"A man convicted of killing the father and sister of his former girlfriend in a fiery attack on the family's Southern California home was sentenced to death on Tuesday. Iftekhar Murtaza, 30, was sentenced for the murders of Jay Dhanak, 56, and his daughter Karishma, 20, in May 2007, the Orange County district attorney's office said. Murtaza was convicted in December 2013 of killing the pair in an attempt to reunite with his then-18-year-old ex-girlfriend Shayona Dhanak. She had ended their relationship citing her Hindu family's opposition to her dating a Muslim. To be executed: Iftekhar Murtaza, 30, was sentenced to death Tuesday for the May 21, 2007 murders of his ex-girlfriend's father and sister and the attempted murder of her mother . Authorities said Murtaza and a friend torched the family's Anaheim Hills home and kidnapped and killed Dhanak's father and sister, leaving their stabbed bodies burning in a park 2 miles from Dhanak's dorm room at the University of California, Irvine. Dhanak's mother, Leela, survived the attack. She was stabbed and left unconscious on a neighbor's lawn. Murtaza was interviewed by police several days later and arrested at a Phoenix airport with a ticket to his native Bangladesh and more than $11,000 in cash. Jurors recommended that Murtaza be sentenced to death for the crimes. Attack: Murtaza torched his ex-girlfriend's family's Orange County home after they broke-up, believing the murders of her family would reunited them . Religious differences: Murtaza dated Shayona Dhanak when she was 18 in 2007. She broke up with him when her Hindu parents allegedly told her they would stop paying her college tuition if she continued to date the Muslim man . Two of his friends were also sentenced to life in prison for the murders, but one of them, Vitaliy Krasnoperov, recently had his conviction overturned on appeal. Authorities said Krasnoperov hatched the plot to kill the Dhanaks with Murtaza and tried to help him hire a hit man. They said another friend, Charles Murphy Jr., helped Murtaza carry out the killings after Dhanak said she planned to go on a date with someone else. During the trial, Murtaza testified that he told many people he wanted to kill the Dhanaks because he was distraught over the breakup, but he said he didn't mean it literally. Didn't do it alone: Two of Murtaza's friends have been convicted in connection to the killings . Killer: Leela Dhanak testified how Iftekhar Murtaza, seen in this August photo, murdered her husband and elder daughter in a failed attempt to win over her younger daughter . Bloodbath: Autopsy reports showed Jayprakash Dhanak (left) suffered 29 stab wounds to his body, while a pathologist testified that Karishma Dhanak (right) was alive when her throat was slit and her body set alight .\"","metadata":{"execution":{"iopub.status.busy":"2024-03-30T02:21:46.252146Z","iopub.execute_input":"2024-03-30T02:21:46.252753Z","iopub.status.idle":"2024-03-30T02:21:46.258521Z","shell.execute_reply.started":"2024-03-30T02:21:46.252720Z","shell.execute_reply":"2024-03-30T02:21:46.257589Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"summarize_text(text3)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T02:21:53.919613Z","iopub.execute_input":"2024-03-30T02:21:53.920430Z","iopub.status.idle":"2024-03-30T02:21:55.311297Z","shell.execute_reply.started":"2024-03-30T02:21:53.920395Z","shell.execute_reply":"2024-03-30T02:21:55.310382Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'Iftekhar Murtaza, 30, was sentenced to death for the 2007 murders of Jay Dhanak, 56, and his daughter Karishma, 20, in May 2007.\\nMurtaza was convicted in December 2013 of killing the pair in an attempt to reunite with his then-'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}